#!/usr/bin/env python3
"""
æµ‹è¯•Whisperæ¨¡å‹åŠ è½½å’ŒåŸºæœ¬åŠŸèƒ½
"""

import whisper
import torch
from pathlib import Path
import time

def test_models():
    """æµ‹è¯•æ¨¡å‹åŠ è½½å’ŒåŸºæœ¬åŠŸèƒ½"""
    
    models_dir = Path("models")
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•Whisperæ¨¡å‹...")
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {models_dir.absolute()}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_files = list(models_dir.glob("*.pt"))
    print(f"\nğŸ“‹ å‘ç° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
    for file in model_files:
        size_mb = file.stat().st_size / (1024 * 1024)
        print(f"   - {file.name} ({size_mb:.1f} MB)")
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    models_to_test = ["base", "turbo"]
    
    for model_name in models_to_test:
        print(f"\nğŸ” æµ‹è¯• {model_name} æ¨¡å‹...")
        
        try:
            start_time = time.time()
            
            # åŠ è½½æ¨¡å‹
            model = whisper.load_model(model_name)
            
            load_time = time.time() - start_time
            print(f"âœ… {model_name} æ¨¡å‹åŠ è½½æˆåŠŸï¼(è€—æ—¶: {load_time:.2f}ç§’)")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            if hasattr(model, 'dims'):
                dims = model.dims
                print(f"   ğŸ“Š æ¨¡å‹å‚æ•°:")
                print(f"      - éŸ³é¢‘çŠ¶æ€ç»´åº¦: {dims.n_audio_state}")
                print(f"      - æ–‡æœ¬çŠ¶æ€ç»´åº¦: {dims.n_text_state}")
                print(f"      - è¯æ±‡è¡¨å¤§å°: {dims.n_vocab}")
                print(f"      - éŸ³é¢‘å±‚æ•°: {dims.n_audio_layer}")
                print(f"      - æ–‡æœ¬å±‚æ•°: {dims.n_text_layer}")
            
            # æ£€æŸ¥è®¾å¤‡
            device = next(model.parameters()).device
            print(f"   ğŸ–¥ï¸ è¿è¡Œè®¾å¤‡: {device}")
            
            # æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼ˆä½¿ç”¨è™šæ‹ŸéŸ³é¢‘æ•°æ®ï¼‰
            print(f"   ğŸµ æµ‹è¯•åŸºæœ¬è½¬å½•åŠŸèƒ½...")
            
            # åˆ›å»ºè™šæ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆ1ç§’çš„é™éŸ³ï¼‰
            sample_rate = 16000
            audio_data = torch.zeros(sample_rate, dtype=torch.float32)
            
            # è¿›è¡Œè½¬å½•æµ‹è¯•
            start_time = time.time()
            result = model.transcribe(audio_data.numpy(), language="zh")
            transcribe_time = time.time() - start_time
            
            print(f"   âœ… è½¬å½•æµ‹è¯•å®Œæˆï¼(è€—æ—¶: {transcribe_time:.2f}ç§’)")
            print(f"   ğŸ“ è½¬å½•ç»“æœ: '{result['text'].strip()}'")
            
        except Exception as e:
            print(f"   âŒ {model_name} æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
            continue
    
    print(f"\nğŸ‰ æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
    
    # æ˜¾ç¤ºä½¿ç”¨å»ºè®®
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"   - baseæ¨¡å‹: é€‚åˆå¿«é€Ÿè½¬å½•ï¼Œå‡†ç¡®æ€§è¾ƒå¥½")
    print(f"   - turboæ¨¡å‹: æœ€æ–°ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå‡†ç¡®æ€§æ›´é«˜")
    print(f"   - æ¨¡å‹å·²ä¿å­˜åœ¨: {models_dir.absolute()}")

if __name__ == "__main__":
    test_models()