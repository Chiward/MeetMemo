#!/usr/bin/env python3
"""
ä¸‹è½½Whisperæ¨¡å‹è„šæœ¬
"""

import os
import sys
import whisper
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.core.config import settings

def download_whisper_model(model_name: str):
    """
    ä¸‹è½½æŒ‡å®šçš„Whisperæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§° (base, large, turbo)
    """
    try:
        print(f"ğŸ”„ å¼€å§‹ä¸‹è½½ {model_name} æ¨¡å‹...")
        
        # è®¾ç½®è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() and settings.WHISPER_DEVICE == "cuda" else "cpu"
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        models_dir = Path(settings.WHISPER_MODELS_DIR)
        models_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {models_dir}")
        
        # ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
        model = whisper.load_model(
            model_name,
            device=device,
            download_root=str(models_dir)
        )
        
        print(f"âœ… {model_name} æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = {
            "base": {"params": "74M", "vram": "~1GB", "speed": "~7x"},
            "large": {"params": "1550M", "vram": "~10GB", "speed": "1x"},
            "turbo": {"params": "809M", "vram": "~6GB", "speed": "~8x"}
        }
        
        if model_name in model_info:
            info = model_info[model_name]
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {info['params']}, æ˜¾å­˜éœ€æ±‚: {info['vram']}, ç›¸å¯¹é€Ÿåº¦: {info['speed']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½ {model_name} æ¨¡å‹å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Whisperæ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥å½“å‰å·²æœ‰çš„æ¨¡å‹
    models_dir = Path(settings.WHISPER_MODELS_DIR)
    if models_dir.exists():
        existing_models = list(models_dir.glob("*.pt"))
        if existing_models:
            print("ğŸ“¦ å½“å‰å·²æœ‰æ¨¡å‹:")
            for model_file in existing_models:
                print(f"  - {model_file.name}")
        else:
            print("ğŸ“¦ å½“å‰æ²¡æœ‰å·²ä¸‹è½½çš„æ¨¡å‹")
    
    print("\nğŸ”„ å¼€å§‹ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹...")
    
    # è¦ä¸‹è½½çš„æ¨¡å‹åˆ—è¡¨
    models_to_download = ["base", "large", "turbo"]
    
    success_count = 0
    total_count = len(models_to_download)
    
    for model_name in models_to_download:
        print(f"\n{'='*30}")
        if download_whisper_model(model_name):
            success_count += 1
        print(f"{'='*30}")
    
    print(f"\nğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
    print(f"âœ… æˆåŠŸ: {success_count}/{total_count}")
    
    if success_count == total_count:
        print("ğŸŠ æ‰€æœ‰æ¨¡å‹ä¸‹è½½æˆåŠŸï¼ç°åœ¨å¯ä»¥åœ¨å‰ç«¯é€‰æ‹©ä¸åŒçš„Whisperæ¨¡å‹äº†ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•ã€‚")
    
    # æ˜¾ç¤ºæœ€ç»ˆçš„æ¨¡å‹ç›®å½•å†…å®¹
    print(f"\nğŸ“ æœ€ç»ˆæ¨¡å‹ç›®å½•å†…å®¹ ({models_dir}):")
    if models_dir.exists():
        for item in models_dir.iterdir():
            if item.is_file():
                size_mb = item.stat().st_size / (1024 * 1024)
                print(f"  - {item.name} ({size_mb:.1f} MB)")

if __name__ == "__main__":
    main()